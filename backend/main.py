import os
from dotenv import load_dotenv
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
import ollama
from openai import OpenAI
import whisper
import io
import base64
from datetime import datetime

# Load environment variables from .env file
load_dotenv()

# Initialize FastAPI app
app = FastAPI()

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)
print(f"[{datetime.now()}] CORS middleware configured.")

# Initialize OpenAI client
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    print("Error: OPENAI_API_KEY not found in .env file.")
    raise ValueError("OPENAI_API_KEY not found in .env file.")
client = OpenAI(api_key=OPENAI_API_KEY)
print(f"[{datetime.now()}] OpenAI client initialized.")

# Initialize local Whisper model
# You might want to choose a different model size (e.g., "small", "medium") based on your system's capabilities
try:
    whisper_model = whisper.load_model("base")
    print(f"[{datetime.now()}] Local Whisper model 'base' loaded successfully.")
except Exception as e:
    print(f"Error loading Whisper model: {e}")
    print("Please ensure 'openai-whisper' is installed and ffmpeg is available.")
    raise

# Store conversation history (simple in-memory for now)
conversation_history = []
print(f"[{datetime.now()}] Conversation history initialized.")

@app.post("/api/chat")
async def chat(request: dict):
    user_message = request.get("message")
    if not user_message:
        raise HTTPException(status_code=400, detail="Message is required")

    print(f"[{datetime.now()}] Received user message: {user_message}")

    # Add user message to history
    conversation_history.append({"role": "user", "content": user_message})

    # Prepare messages for Ollama, including system prompt and history
    messages_for_ollama = [
        {
            'role': 'system',
            'content': 'You are Jarvis, a friendly and knowledgeable AI assistant specializing in technology and tech news. Your goal is to be a helpful mentor, providing clear and concise information. If a question is outside the scope of technology, politely decline and gently guide the conversation back to tech-related topics. For example, if asked about cooking, you could say, "While I can not help with recipes, I can tell you about the latest in smart kitchen technology." Maintain a futuristic and encouraging tone.'
        }
    ] + conversation_history

    print(f"[{datetime.now()}] Sending messages to Ollama:")
    for msg in messages_for_ollama:
        print(f"  {msg['role']}: {msg['content']}")

    try:
        # Stream the response from Ollama
        stream = ollama.chat(
            model='llama3',  # Or another model of your choice
            messages=messages_for_ollama,
            stream=True,
        )

        response_content = ""
        for chunk in stream:
            content = chunk['message']['content']
            response_content += content
            print(f"[{datetime.now()}] Ollama chunk: {content}")

        print(f"[{datetime.now()}] Full Ollama response: {response_content}")

        # Add assistant response to history
        conversation_history.append({"role": "assistant", "content": response_content})

        # Generate speech using OpenAI TTS
        print(f"[{datetime.now()}] Generating speech for: {response_content[:50]}...") # Log first 50 chars
        speech_response = client.audio.speech.create(
            model="tts-1",
            voice="onyx", # You can choose other voices like "alloy", "echo", "fable", "nova", "shimmer"
            input=response_content,
        )
        print(f"[{datetime.now()}] Speech generated by OpenAI TTS.")

        # Read the audio content into a BytesIO object
        audio_buffer = io.BytesIO()
        for chunk in speech_response.iter_bytes(chunk_size=4096):
            audio_buffer.write(chunk)
        audio_buffer.seek(0) # Rewind to the beginning

        # Encode audio to base64 for sending over JSON
        encoded_audio = base64.b64encode(audio_buffer.read()).decode('utf-8')
        print(f"[{datetime.now()}] Audio encoded to base64.")

        return {"response": response_content, "audio": encoded_audio}
    except Exception as e:
        print(f"Error in /api/chat: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/transcribe")
async def transcribe_audio(audio_file: UploadFile = File(...)):
    print(f"[{datetime.now()}] Received audio file for transcription: {audio_file.filename}")
    if not audio_file.content_type.startswith("audio/"):
        raise HTTPException(status_code=400, detail="Invalid file type. Only audio files are supported.")

    try:
        import tempfile
        import numpy as np

        audio_bytes = await audio_file.read()

        # Create a temporary file to save the audio
        with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as tmp_audio_file:
            tmp_audio_file.write(audio_bytes)
            tmp_audio_path = tmp_audio_file.name

        print(f"[{datetime.now()}] Saved audio to temporary file: {tmp_audio_path}")

        try:
            # Load the audio using whisper's utility, which returns a numpy array
            audio_np = whisper.load_audio(tmp_audio_path)
            print(f"[{datetime.now()}] Audio loaded into NumPy array.")

            print(f"[{datetime.now()}] Transcribing audio with Whisper...")
            result = whisper_model.transcribe(audio_np)
        finally:
            # Clean up the temporary file
            os.remove(tmp_audio_path)
            print(f"[{datetime.now()}] Removed temporary file: {tmp_audio_path}")
        transcribed_text = result["text"]
        print(f"[{datetime.now()}] Transcription complete: {transcribed_text}")

        return {"transcript": transcribed_text}
    except Exception as e:
        print(f"Error in /api/transcribe: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    print("Starting Uvicorn server...")
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)